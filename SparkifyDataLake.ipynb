{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bb59c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329078b2c71b43078eed46f7bfff7a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1665543968837_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-92-188.ec2.internal:20888/proxy/application_1665543968837_0003/\" class=\"emr-proxy-link\" emr-resource=\"j-3CBM1RHP74SUJ\n",
       "\" application-id=\"application_1665543968837_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-93-20.ec2.internal:8042/node/containerlogs/container_1665543968837_0003_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col, desc, substring\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776cfae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74fd58b25d24dd68285fd117bd10e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        spark: spark session\n",
    "        input_data: Path to input data\n",
    "        output_data: Path to output data\n",
    "    Returns:\n",
    "        Outputs the songs_table and the artists_table to S3\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\" # replace A/A/A with */*/* when submitting\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).dropDuplicates()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write\\\n",
    "    .partitionBy(\"year\", \"artist_id\")\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet(os.path.join(output_data, 'songs'))\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    cols = ['artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']\n",
    "    cols = [col + ' as ' + col[7:] for col in cols]\n",
    "    artists_table = df.selectExpr('artist_id', *cols) \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write\\\n",
    "    .parquet(os.path.join(output_data, 'songs'),\n",
    "             mode='overwrite')\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        spark: spark session\n",
    "        input_data: Path to input data\n",
    "        output_data: Path to output data\n",
    "    Returns:\n",
    "        Outputs the users_table, time_table and the songplays_table into S3\n",
    "    \"\"\"  \n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + \"log_data/*/*/*.json\" \n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(\" page = 'NextSong'\")\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.selectExpr(\"userId as user_id\", \n",
    "                                 \"firstName as first_name\",\n",
    "                                 \"lastName as last_name\",\n",
    "                                 \"gender\", \n",
    "                                 \"level\").dropDuplicates()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(os.path.join(output_data, \"users\")\n",
    "                              , mode=\"overwrite\")\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda ts: datetime.fromtimestamp(ts/1000), TimestampType())\n",
    "    df = df.withColumn('start_time', get_timestamp('ts'))\n",
    "    \n",
    "#     # create datetime column from original timestamp column\n",
    "#     get_datetime = udf()\n",
    "#     df = \n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('start_time')\n",
    "    # list of functions\n",
    "    funcs = [F.hour, F.dayofmonth, F.weekofyear, F.month, F.year, F.dayofweek]\n",
    "    cols = ['hour', 'day', 'week', 'month', 'year', 'weekday']\n",
    "    time_cols = [(col, func('start_time'))for func, col in zip(funcs, cols)]\n",
    "    for col in time_cols:\n",
    "        time_table = time_table.withColumn(*col) \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\")\\\n",
    "    .parquet(os.path.join(output_data, \"times\"))\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json(os.path.join(input_data, 'song_data', 'A', 'A', 'A'))\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    df = df.orderBy('ts')\n",
    "    df = df.withColumn('songplay_id', F.monotonically_increasing_id())\n",
    "\n",
    "    song_df.createOrReplaceTempView('songs')\n",
    "    df.createOrReplaceTempView('events')\n",
    "\n",
    "    # include year and month to allow parquet partitioning\n",
    "    songplays_table = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            e.songplay_id,\n",
    "            e.start_time,\n",
    "            e.userId as user_id,\n",
    "            e.level,\n",
    "            s.song_id,\n",
    "            s.artist_id,\n",
    "            e.sessionId as session_id,\n",
    "            e.location,\n",
    "            e.userAgent as user_agent,\n",
    "            year(e.start_time) as year,\n",
    "            month(e.start_time) as month\n",
    "        FROM events e\n",
    "        LEFT JOIN songs s ON\n",
    "            e.song = s.title AND\n",
    "            e.artist = s.artist_name\n",
    "    \"\"\") \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(os.path.join(output_data, 'songplays'), \n",
    "                                  partitionBy=['year', 'month'],\n",
    "                                  mode=\"overwrite\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7032a0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a6e3d6bbd241198230438feffe82ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"./Results/\"\n",
    "#     input_data, output_data = './data/', './output/'  # Uncomment for local mode\n",
    "    \n",
    "process_song_data(spark, input_data, output_data)    \n",
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5946320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
